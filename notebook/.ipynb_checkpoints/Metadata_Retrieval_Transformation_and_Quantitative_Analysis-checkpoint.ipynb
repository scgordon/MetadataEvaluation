{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting and analyzing EML records uploaded to DataONE by individual LTER sites, 2005-2018: MetadataRetrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two flaws were discovered in the course of looking at LTER and other membernodes; LTER is actually made of many different and semi-autonomous sites, and sampling from the entire collection doesnt give an accurate picture of collection improvement as a factor of experience with the LTER Recommendation. To address this I have made a workflow that queries DataONE for specific sites and creates directories of records uploaded during a year. This way we can look at the individual site's metadata usage evolution with respect to the elements in the LTER Recommendation Levels and better address the question of whether a community can successfully leverage adoption of a metadata recommendation to promote robust and complete metadata records that achieve a variety of documentation needs. After talking with a data manager at EDI I learned I could identify records by site using part of the record identifier. While it may make sense at a later date to include every record that was currently in the collection for a specific date range, we decided to focus on \n",
    "\n",
    "This notebook is firstly a way to create some markdown with SOLR requests and links to scripts and transforms utilized to build the metadata collections and to process the records for validity and eml: namespace consistency with evaluation transforms. Later versions may create cell magics to replace the bash used in my Mac's Terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The MacBook Pro I used for the analysis:\n",
    "\n",
    "##### Hardware Overview:\n",
    "\n",
    "*  Model Name:\tMacBook Pro\n",
    "*  Model Identifier:\tMacBookPro11,1\n",
    "*  Processor Name:\tIntel Core i7\n",
    "*  Processor Speed:\t3 GHz\n",
    "*  Number of Processors:\t1\n",
    "*  Total Number of Cores:\t2\n",
    "*  L2 Cache (per Core):\t256 KB\n",
    "*  L3 Cache:\t4 MB\n",
    "*  Memory:\t16 GB\n",
    "*  Boot ROM Version:\tMBP111.0142.B00\n",
    "*  SMC Version (system):\t2.16f68\n",
    "*  Serial Number (system):\tC02P607XG3QT\n",
    "*  Hardware UUID:\t23B57CF1-5206-536F-B66B-CF69B31D234B\n",
    "  \n",
    "#####  System Software Overview:\n",
    "\n",
    "*  System Version:\tmacOS 10.13.3 (17D102)\n",
    "*  Kernel Version:\tDarwin 17.4.0\n",
    "*  Boot Volume:\tMacintosh HD\n",
    "*  Boot Mode:\tNormal\n",
    "*  Computer Name:\tbear\n",
    "*  User Name:\tSean Gordon (scgordon)\n",
    "*  Secure Virtual Memory:\tEnabled\n",
    "*  System Integrity Protection:\tEnabled\n",
    "*  Time since boot:\t12 days 4:39\n",
    "\n",
    "##### Python Overview:\n",
    "\n",
    "Python 3.6.4 :: Anaconda, Inc.\n",
    "\n",
    "*  alabaster (0.7.10)\n",
    "*  anaconda-client (1.6.9)\n",
    "*  anaconda-navigator (1.7.0)\n",
    "*  anaconda-project (0.8.2)\n",
    "*  appnope (0.1.0)\n",
    "*  appscript (1.0.1)\n",
    "*  asn1crypto (0.24.0)\n",
    "*  astroid (1.6.1)\n",
    "*  astropy (2.0.3)\n",
    "*  astunparse (1.5.0)\n",
    "*  attrs (17.4.0)\n",
    "*  Babel (2.5.3)\n",
    "*  backports.shutil-get-terminal-size (1.0.0)\n",
    "*  beautifulsoup4 (4.6.0)\n",
    "*  bitarray (0.8.1)\n",
    "*  bkcharts (0.2)\n",
    "*  blaze (0.11.3)\n",
    "*  bleach (2.1.1)\n",
    "*  bokeh (0.12.13)\n",
    "*  boto (2.48.0)\n",
    "*  Bottleneck (1.2.1)\n",
    "*  certifi (2017.7.27.1)\n",
    "*  cffi (1.11.4)\n",
    "*  chardet (3.0.4)\n",
    "*  click (6.7)\n",
    "*  cloudpickle (0.5.2)\n",
    "*  clyent (1.2.2)\n",
    "*  colorama (0.3.9)\n",
    "*  conda (4.4.10)\n",
    "*  conda-build (3.4.1)\n",
    "*  conda-verify (2.0.0)\n",
    "*  contextlib2 (0.5.5)\n",
    "*  cryptography (2.1.4)\n",
    "*  cycler (0.10.0)\n",
    "*  Cython (0.27.3)\n",
    "*  cytoolz (0.9.0)\n",
    "*  dask (0.16.1)\n",
    "*  datashape (0.5.4)\n",
    "*  decorator (4.2.1)\n",
    "*  distributed (1.20.2)\n",
    "*  docutils (0.14)\n",
    "*  entrypoints (0.2.3)\n",
    "*  et-xmlfile (1.0.1)\n",
    "*  fastcache (1.0.2)\n",
    "*  filelock (2.0.13)\n",
    "*  Flask (0.12.2)\n",
    "*  Flask-Cors (3.0.3)\n",
    "*  geojson (2.3.0)\n",
    "*  gevent (1.2.2)\n",
    "*  glob2 (0.6)\n",
    "*  gmpy2 (2.0.8)\n",
    "*  greenlet (0.4.12)\n",
    "*  h5py (2.7.1)\n",
    "*  heapdict (1.0.0)\n",
    "*  html5lib (1.0b10)\n",
    "*  idna (2.6)\n",
    "*  imageio (2.2.0)\n",
    "*  imagesize (0.7.1)\n",
    "*  ipykernel (4.8.0)\n",
    "*  ipython (6.2.1)\n",
    "*  ipython-genutils (0.2.0)\n",
    "*  ipywidgets (7.1.1)\n",
    "*  isort (4.2.15)\n",
    "*  itsdangerous (0.24)\n",
    "*  jdcal (1.3)\n",
    "*  jedi (0.11.1)\n",
    "*  Jinja2 (2.10)\n",
    "*  jsonschema (2.6.0)\n",
    "*  jupyter (1.0.0)\n",
    "*  jupyter-client (5.1.0)\n",
    "*  jupyter-console (5.2.0)\n",
    "*  jupyter-core (4.4.0)\n",
    "*  jupyter-kernel-gateway (2.0.1)\n",
    "*  jupyterlab (0.31.5)\n",
    "*  jupyterlab-launcher (0.10.2)\n",
    "*  lazy-object-proxy (1.3.1)\n",
    "*  llvmlite (0.21.0)\n",
    "*  locket (0.2.0)\n",
    "*  lxml (4.1.1)\n",
    "*  Markdown (2.6.11)\n",
    "*  MarkupSafe (1.0)\n",
    "*  matplotlib (2.1.2)\n",
    "*  mccabe (0.6.1)\n",
    "*  mistune (0.8.3)\n",
    "*  mpld3 (0.3)\n",
    "*  mpmath (1.0.0)\n",
    "*  msgpack-python (0.5.1)\n",
    "*  multipledispatch (0.4.9)\n",
    "*  navigator-updater (0.1.0)\n",
    "*  nbconvert (5.3.1)\n",
    "*  nbformat (4.4.0)\n",
    "*  networkx (2.1)\n",
    "*  nltk (3.2.5)\n",
    "*  nose (1.3.7)\n",
    "*  notebook (5.2.0)\n",
    "*  numba (0.36.2)\n",
    "*  numexpr (2.6.4)\n",
    "*  numpy (1.14.0)\n",
    "*  numpydoc (0.7.0)\n",
    "*  odo (0.5.1)\n",
    "*  olefile (0.45.1)\n",
    "*  openpyxl (2.4.10)\n",
    "*  packaging (16.8)\n",
    "*  pandas (0.22.0)\n",
    "*  pandocfilters (1.4.2)\n",
    "*  parso (0.1.1)\n",
    "*  partd (0.3.8)\n",
    "*  path.py (10.5)\n",
    "*  pathlib2 (2.3.0)\n",
    "*  patsy (0.5.0)\n",
    "*  pep8 (1.7.1)\n",
    "*  pexpect (4.3.1)\n",
    "*  pickleshare (0.7.4)\n",
    "*  Pillow (5.0.0)\n",
    "*  pip (9.0.3)\n",
    "*  pixiedust (1.1.9)\n",
    "*  pkginfo (1.4.1)\n",
    "*  pluggy (0.6.0)\n",
    "*  ply (3.10)\n",
    "*  prompt-toolkit (1.0.15)\n",
    "*  psutil (5.4.3)\n",
    "*  ptyprocess (0.5.2)\n",
    "*  py (1.5.2)\n",
    "*  pycodestyle (2.3.1)\n",
    "*  pycosat (0.6.3)\n",
    "*  pycparser (2.18)\n",
    "*  pycrypto (2.6.1)\n",
    "*  pycurl (7.43.0.1)\n",
    "*  pyflakes (1.6.0)\n",
    "*  Pygments (2.2.0)\n",
    "*  pylint (1.8.2)\n",
    "*  pyodbc (4.0.22)\n",
    "*  pyOpenSSL (17.5.0)\n",
    "*  pyparsing (2.2.0)\n",
    "*  PySocks (1.6.7)\n",
    "*  pytest (3.3.2)\n",
    "*  python-dateutil (2.6.1)\n",
    "*  pytz (2017.3)\n",
    "*  PyWavelets (0.5.2)\n",
    "*  PyYAML (3.12)\n",
    "*  pyzmq (16.0.3)\n",
    "*  QtAwesome (0.4.4)\n",
    "*  qtconsole (4.3.1)\n",
    "*  QtPy (1.3.1)\n",
    "*  requests (2.18.4)\n",
    "*  rope (0.10.7)\n",
    "*  ruamel-yaml (0.15.35)\n",
    "*  scikit-image (0.13.1)\n",
    "*  scikit-learn (0.19.1)\n",
    "*  scipy (1.0.0)\n",
    "*  seaborn (0.8.1)\n",
    "*  Send2Trash (1.4.2)\n",
    "*  setuptools (36.6.0)\n",
    "*  simplegeneric (0.8.1)\n",
    "*  singledispatch (3.4.0.3)\n",
    "*  six (1.11.0)\n",
    "*  snowballstemmer (1.2.1)\n",
    "*  sortedcollections (0.5.3)\n",
    "*  sortedcontainers (1.5.9)\n",
    "*  Sphinx (1.6.6)\n",
    "*  sphinxcontrib-websupport (1.0.1)\n",
    "*  spyder (3.2.6)\n",
    "*  SQLAlchemy (1.2.1)\n",
    "*  statsmodels (0.8.0)\n",
    "*  sympy (1.1.1)\n",
    "*  tables (3.4.2)\n",
    "*  tblib (1.3.2)\n",
    "*  terminado (0.8.1)\n",
    "*  testpath (0.3.1)\n",
    "*  toolz (0.9.0)\n",
    "*  tornado (4.5.2)\n",
    "*  traitlets (4.3.2)\n",
    "*  typing (3.6.2)\n",
    "*  unicodecsv (0.14.1)\n",
    "*  urllib3 (1.22)\n",
    "*  wcwidth (0.1.7)\n",
    "*  webencodings (0.5.1)\n",
    "*  Werkzeug (0.14.1)\n",
    "*  wheel (0.30.0)\n",
    "*  widgetsnbextension (3.1.0)\n",
    "*  wrapt (1.10.11)\n",
    "*  xlrd (1.1.0)\n",
    "*  XlsxWriter (1.0.2)\n",
    "*  xlwings (0.11.5)\n",
    "*  xlwt (1.2.0)\n",
    "*  zict (0.1.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I obtained a list of sites from https://lternet.edu/site/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Andrews Forest LTER (AND)\n",
    "* Arctic LTER (ARC)\n",
    "* Baltimore Ecosystem Study (BES)\n",
    "* Beaufort Lagoon Ecosystem LTER (BLE)\n",
    "* Bonanza Creek LTER (BNZ)\n",
    "* California Current Ecosystem LTER (CCE)\n",
    "* Cedar Creek Ecosystem Science Reserve (CDR)\n",
    "* Central Arizona â€“ Phoenix LTER (CAP)\n",
    "* Coweeta LTER (CWT)\n",
    "* Florida Coastal Everglades LTER (FCE)\n",
    "* Georgia Coastal Ecosystems LTER (GCE)\n",
    "* Harvard Forest LTER (HFR)\n",
    "* Hubbard Brook LTER (HBR)\n",
    "* Jornada Basin LTER (JRN)\n",
    "* Kellogg Biological Station LTER (KBS)\n",
    "* Konza Prairie LTER (KNZ)\n",
    "* LTER Network (NWK)\n",
    "* LTER Network Communications Office (NCO)\n",
    "* Luquillo LTER (LUQ)\n",
    "* McMurdo Dry Valleys LTER (MCM)\n",
    "* Moorea Coral Reef LTER (MCR)\n",
    "* Niwot Ridge LTER (NWT)\n",
    "* North Temperate Lakes LTER (NTL)\n",
    "* Northeast U.S. Shelf (NES)\n",
    "* Northern Gulf of Alaska (NGA)\n",
    "* Palmer Antarctica LTER (PAL)\n",
    "* Plum Island Ecosystems LTER (PIE)\n",
    "* Santa Barbara Coastal LTER (SBC)\n",
    "* Sevilleta LTER (SEV)\n",
    "* Virginia Coast Reserve LTER (VCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataONE SOLR queries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I queried DataONE using their SOLR index: https://cn.dataone.org/cn/v2/query/solr  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I determined how many metadata records were in the LTER collection at DataONE: http://cn.dataone.org/cn/v2/query/solr/?q=formatType:METADATA+AND+authoritativeMN:*LTER&rows=0\n",
    "\n",
    "On Feb first the LTER network had 75819 metadata records in DataONE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```xml\n",
    "<response>\n",
    "<lst name=\"responseHeader\">\n",
    "<int name=\"status\">0</int>\n",
    "<int name=\"QTime\">4</int>\n",
    "<lst name=\"params\">\n",
    "<str name=\"q\">formatType:METADATA AND authoritativeMN:*LTER</str>\n",
    "<str name=\"rows\">0</str>\n",
    "</lst>\n",
    "</lst>\n",
    "<result name=\"response\" numFound=\"75819\" start=\"0\"/>\n",
    "</response> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I retrieved total collection size for each site using the following query and replacing the \"X\" with the three character site identifier from the above list in lower case:\n",
    "\n",
    "http://cn.dataone.org/cn/v2/query/solr/?q=formatType:METADATA+AND+authoritativeMN:*LTER&fl=identifier,dateUploaded,datePublished,authoritativeMN,obsoletes,obsoletedBy,archived,dataUrl&rows=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Andrews Forest LTER (AND) = 423\n",
    "* Arctic LTER (ARC) = 4429\n",
    "* Baltimore Ecosystem Study (BES) = 4048\n",
    "* Beaufort Lagoon Ecosystem (BLE) = 0\n",
    "* Bonanza Creek LTER (BNZ) = 5446\n",
    "* California Current Ecosystem LTER (CCE) = 210\n",
    "* Cedar Creek Ecosystem Science Reserve (CDR) = 2985\n",
    "* Central Arizona â€“ Phoenix LTER (CAP) = 1757\n",
    "* Coweeta LTER (CWT) = 435\n",
    "* Florida Coastal Everglades LTER (FCE) = 2172\n",
    "* Georgia Coastal Ecosystems LTER (GCE) = 3728\n",
    "* Harvard Forest LTER (HFR) = 2177\n",
    "* Hubbard Brook LTER (HBR) = 394\n",
    "* Jornada Basin LTER (JRN) = 451\n",
    "* Kellogg Biological Station LTER (KBS) = 908\n",
    "* Konza Prairie LTER (KNZ) = 401\n",
    "* LTER Network (NWK) = 6\n",
    "* LTER Network Communications Office (NCO) = 0\n",
    "* Luquillo LTER (LUQ) = 491\n",
    "* McMurdo Dry Valleys LTER (MCM) = 892\n",
    "* Moorea Coral Reef LTER (MCR) = 432\n",
    "* Niwot Ridge LTER (NWT) = 411\n",
    "* North Temperate Lakes LTER (NTL) =1292\n",
    "* Northeast U.S. Shelf (NES) = 0\n",
    "* Northern Gulf of Alaska (NGA) = 0 \n",
    "* Palmer Antarctica LTER (PAL) = 296\n",
    "* Plum Island Ecosystems LTER (PIE) = 1132\n",
    "* Santa Barbara Coastal LTER (SBC) = 1073\n",
    "* Sevilleta LTER (SEV) = 1112\n",
    "* Shortgrass Steppe (No longer funded by NSF LTER) (SGS) = 504\n",
    "* Virginia Coast Reserve LTER (VCR) = 1763"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something doesn't add up!\n",
    "\n",
    "Grand total found at sites: 38368\n",
    "\n",
    "Total DataONE gave  = 75819\n",
    "\n",
    "That means almost half the collection's identifiers did not follow the convention or were otherwise unavailable.\n",
    "\n",
    "When I was using a regex instead of a phrase search of the site identifier I found LTER-Landsat-LEDAPS while searching for a string that contained and.\n",
    "\n",
    "LTER-Landsat-LEDAPS had 20593 records.\n",
    "\n",
    "Leaving a total of 16858 records still unaccounted for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "38368 records is still a lot of metadata and most sites have hundreds if not thousands of records. Are the record uploads spread throughout the years in question?\n",
    "\n",
    "Let's look at the relevant lst element from the response for Andrews Forest LTER (AND):  \n",
    "\n",
    "http://cn.dataone.org/cn/v2/query/solr/?q=formatType:METADATA+AND+authoritativeMN:*LTER+AND+identifier:%27-lter-and%27&fl=dateUploaded,datePublished,dataUrl&rows=0&sort=dateUploaded+asc&facet=true&facet.missing=true&facet.limit=-1&facet.range=dateUploaded&facet.range.start=2005-01-01T00:00:00Z&facet.range.end=2018-12-31T23:59:59.999Z&facet.range.gap=%2B1YEAR&wt=xml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```xml \n",
    "<result name=\"response\" numFound=\"424\" start=\"0\"/>\n",
    "<lst name=\"facet_counts\">\n",
    "<lst name=\"facet_queries\"/>\n",
    "<lst name=\"facet_fields\"/>\n",
    "<lst name=\"facet_dates\"/>\n",
    "<lst name=\"facet_ranges\">\n",
    "<lst name=\"dateUploaded\">\n",
    "<lst name=\"counts\">\n",
    "<int name=\"2005-01-01T00:00:00Z\">184</int>\n",
    "<int name=\"2006-01-01T00:00:00Z\">14</int>\n",
    "<int name=\"2007-01-01T00:00:00Z\">19</int>\n",
    "<int name=\"2008-01-01T00:00:00Z\">6</int>\n",
    "<int name=\"2009-01-01T00:00:00Z\">1</int>\n",
    "<int name=\"2010-01-01T00:00:00Z\">1</int>\n",
    "<int name=\"2011-01-01T00:00:00Z\">3</int>\n",
    "<int name=\"2012-01-01T00:00:00Z\">3</int>\n",
    "<int name=\"2013-01-01T00:00:00Z\">9</int>\n",
    "<int name=\"2014-01-01T00:00:00Z\">12</int>\n",
    "<int name=\"2015-01-01T00:00:00Z\">123</int>\n",
    "<int name=\"2016-01-01T00:00:00Z\">33</int>\n",
    "<int name=\"2017-01-01T00:00:00Z\">15</int>\n",
    "<int name=\"2018-01-01T00:00:00Z\">1</int>\n",
    "</lst> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some years have significantly more activity but there is at least some demonstration of complete metadata uploaded in each of the 14 years. Other sites will have years they do not upload metadata and the directory structure should reflect that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Excel to create a XML retrival script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I can create a solr xml response for each site that lists a download url and a date uploaded, I can create the proper directory structures and download the directories.\n",
    "\n",
    "To do this I took the list of sites and the counts of metadata and imported it into Excel. In Excel I used a formula to construct the solr commands that would download the required information and wrapped it in a curl to save as xml. The list is vertically oriented so the rest of the site identifiers and metadata record count are in ```F3:G33``` and the resulting curl commands are listed in ```J2:J33``` of the BuildcURLsALL worksheet in [LTERsiteEvolution.xlsx](../scripts/LTERsiteEvolution.xlsx)\n",
    "\n",
    "``` =$A$1&F2&$B$1&G2+1000&$C$1&UPPER($F2)&E$1&D$1 ```\n",
    "\n",
    "Where:\n",
    "\n",
    "* A1\t```curl \"http://cn.dataone.org/cn/v2/query/solr/?q=formatType:METADATA+AND+authoritativeMN:*LTER+AND++AND+identifier:%27-lter- ```\n",
    "* B1\t```%27&fl=dateUploaded,datePublished,dataUrl&rows=```\n",
    "* C1    ```&sort=dateUploaded+asc&facet=true&facet.missing=true&facet.limit=-1&facet.range=dateUploaded&facet.range.start=2005-01-01T00:00:00Z&facet.range.end=2018-12-31T23:59:59.999Z&facet.range.gap=%2B1YEAR&wt=xml\" > ```\n",
    "* F2    The three letter site identifier, lowercase\n",
    "* E1\t```.xml```\n",
    "* G2    The count of records in the SOLR index on Feb first (adding 1000 in the formula to ensure all records in the past month were collected through the resulting query as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting in a list I could use to retrieve the XML I was going to need to transform in order to organize my download directories in preparation for evaluation. \n",
    "\n",
    "Here are the curl commands: [RecordQueries.sh](../scripts/RecordQueries.sh)\n",
    "\n",
    "You may notice some rows with \"#Value!\". Some sites did not appear in the identifiers or only had a few records. Finally Landsat was removed as the collection was all uploaded in 2013.\n",
    "\n",
    "Sites removed from evaluation:\n",
    "* Beaufort Lagoon Ecosystem (BLE) = 0\n",
    "* LTER Network (NWK) = 6\n",
    "* LTER Network Communications Office (NCO) = 0\n",
    "* Northeast U.S. Shelf (NES) = 0\n",
    "* Northern Gulf of Alaska (NGA) = 0 \n",
    "* Landsat (LANDSAT) = 20593\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata about metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resultant XML for the sites to be evaluated are in this directory: [solrMetadata](../solrMetadata/) \n",
    "\n",
    "Below is a snippet of the response. Each doc element is a record. I was interested in the date published field as well as a different way to organize, but as you can see not every record has that information. Thus we need to organize the records by the site they were created at and the date they were uploaded to DataONE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```xml\n",
    "<response>\n",
    "    <lst name=\"responseHeader\">\n",
    "        <int name=\"status\">0</int>\n",
    "        <int name=\"QTime\">46</int>\n",
    "        <lst name=\"params\">\n",
    "            <str name=\"facet.range\">dateUploaded</str>\n",
    "            <str name=\"q\">formatType:METADATA AND authoritativeMN:*LTER AND identifier:'-lter-arc'</str>\n",
    "            <str name=\"facet.limit\">-1</str>\n",
    "            <str name=\"facet.range.gap\">+1YEAR</str>\n",
    "            <str name=\"fl\">dateUploaded,datePublished,dataUrl</str>\n",
    "            <str name=\"facet.missing\">true</str>\n",
    "            <str name=\"sort\">dateUploaded asc</str>\n",
    "            <str name=\"rows\">5429</str>\n",
    "            <str name=\"facet\">true</str>\n",
    "            <str name=\"wt\">xml</str>\n",
    "            <str name=\"facet.range.start\">2001-01-01T00:00:00Z</str>\n",
    "            <str name=\"facet.range.end\">2018-12-31T23:59:59.999Z</str>\n",
    "        </lst>\n",
    "    </lst>\n",
    "    <result name=\"response\" numFound=\"4435\" start=\"0\">\n",
    "        <doc>\n",
    "            <date name=\"dateUploaded\">2005-07-27T23:00:00Z</date>\n",
    "            <str name=\"dataUrl\">https://cn.dataone.org/cn/v2/resolve/doi%3A10.6073%2FAA%2Fknb-lter-arc.584.1</str>\n",
    "        </doc>\n",
    "        <doc>\n",
    "            <date name=\"dateUploaded\">2005-07-27T23:00:00Z</date>\n",
    "            <str name=\"dataUrl\">https://cn.dataone.org/cn/v2/resolve/knb-lter-arc.994.10</str>\n",
    "            <date name=\"datePublished\">2000-01-01T00:00:00Z</date>\n",
    "        </doc>\n",
    "        <doc>\n",
    "            <date name=\"dateUploaded\">2005-07-27T23:00:00Z</date>\n",
    "            <str name=\"dataUrl\">https://cn.dataone.org/cn/v2/resolve/knb-lter-arc.1406.7</str>\n",
    "            <date name=\"datePublished\">1999-01-01T00:00:00Z</date>\n",
    "        </doc>\n",
    "        <doc>\n",
    "            <date name=\"dateUploaded\">2005-07-27T23:00:00Z</date>\n",
    "            <str name=\"dataUrl\">https://cn.dataone.org/cn/v2/resolve/doi%3A10.6073%2FAA%2Fknb-lter-arc.1388.2</str>\n",
    "        </doc>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using XSL to transform the metadata about metadata into the EML collections we want to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a XML document for each site that lists a download url and a date uploaded for each record. I need to create a shell script for each collection that makes sure the proper directory structure for the record exists and saves the retrieved XML there. I used XSL to create the shell scripts from the XML: [createSiteSH.xsl](../scripts/createSiteSH.xsl) \n",
    "\n",
    "The resulting scripts are named for the site identifier (eg: [AND.sh](../scripts/AND.sh)) and located in the [../scripts](../scripts/) directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running these scripts results in directories being created for each site that had records with subdirectories for any year records were uploaded, with subdirectories /EML/xml. For example, the records for AND from 2005 were curled to [../metadata/AND/2005/EML/xml](../metadata/AND/2005/EML/xml) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning metadata: Normalizing the namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the metadata is downloaded and organized, we need to align all the namespace locations for EML. Over the 14 years in question EML has gone through several versions. The evaluation transform recognizes the location for the 2.1.1 version on the namespace location, so records with xmlns:eml=\"eml://ecoinformatics.org/eml-2.1.0\" and xmlns:eml=\"eml://ecoinformatics.org/eml-2.0.1\" recieved replacement a location to xmlns:eml=\"eml://ecoinformatics.org/eml-2.1.1\"\n",
    "\n",
    "This allows the files to be evaluated for [conceptual content](http://wiki.esipfed.org/index.php/Documentation_Terminology#Concepts). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above didnt need to happen because of the change in style of transformation...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate using XSL and python to create data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using functions developed in the module [metadataEvaluation.py](../scripts/metadataEvaluation.py) and scripts to run transforms in a private repository data for xpath occurrence as well as all content in the records is created in subdirectories for each site in [../data](../data). \n",
    "\n",
    "This script is called [SyntacticSugar.py](../scripts/SyntacticSugar.py) and it requires two lists to run. The first list [ListOfCollections.csv](../scripts/ListOfCollections.csv) takes three arguments: the site abbreviation, the year in question, and the dialect and returns 2 data files by running two functions from metadataEvaluation. \n",
    "\n",
    "The second list [ListOfOrganizations.csv](../scripts/ListOfOrganizations.csv) creates pivot tables for each organization directory. It takes the occurrence data tables and creates comparisons of each collection in the organization directory. Finally it creates an Excel Workbook for each organization with each original combined csv included as a worksheet. additional analysis and visualization occurs in additional worksheets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One shell to run them all, errors emerge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[batch.sh](../scripts/batch.sh) creates a call to [GetData.sh](../scripts/GetData.sh) for each site/year. During the process of running the script it was discovered that 93 of the records contained invalid xml due to a server error, causing those collections to not be evaluated properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurl the 93, re run [batch.sh](../scripts/batch.sh) after adding combination functions to create combined data files for the site also fix the transform location for file to avoid moveup.sh\n",
    "\n",
    "We need a list of the records with errors. I mined the error reports from terminal using a regex in Sublime for the filepaths, which I can use to identify the proper curls to rerun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately upon reruning the workflow it was discovered that the Terminal reports were truncated. New methods needed to be created. Through exploration of the collections using the application Oxygen, it was discovered that records showing up in a Terminal error were not well-formed XML.\n",
    "\n",
    "Thus the solution was to check for XML well-formedness. The incorrect XML responses contained partial records and an error report. The error report was used to create a new set of cURLs to get the rest of the records. In the future metadataEvaluation.getURLs function would take over this work, reducing the amount of incorrect data retrievals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i '../scripts/SyntacticSugar.py'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
